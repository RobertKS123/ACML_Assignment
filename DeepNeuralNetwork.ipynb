{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, classification_report, roc_auc_score, roc_curve, auc\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.0</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.0</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.0</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step  type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0     1     2   9839.64  C1231006815       170136.0       160296.36   \n",
       "1     1     2   1864.28  C1666544295        21249.0        19384.72   \n",
       "2     1     4    181.00  C1305486145          181.0            0.00   \n",
       "3     1     1    181.00   C840083671          181.0            0.00   \n",
       "4     1     2  11668.14  C2048537720        41554.0        29885.86   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
       "0  M1979787155             0.0             0.0        0               0  \n",
       "1  M2044282225             0.0             0.0        0               0  \n",
       "2   C553264065             0.0             0.0        1               0  \n",
       "3    C38997010         21182.0             0.0        1               0  \n",
       "4  M1230701703             0.0             0.0        0               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./Data/onlinefraud.csv')\n",
    "# Convert the 'type' attribute to a numerical one\n",
    "data[\"type\"] = data[\"type\"].map({\"CASH_OUT\": 1, \"PAYMENT\": 2, \"CASH_IN\": 3, \"TRANSFER\": 4, \"DEBIT\": 5})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"nameOrig\", \"nameDest\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shapes - X: (5090096, 8) y: (5090096,)\n",
      "Test set shapes - X: (636262, 8) y: (636262,)\n",
      "Validation set shapes - X: (636262, 8) y: (636262,)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data randomly\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Set the target variable (y) and input features (X)\n",
    "target_variable = 'isFraud'\n",
    "input_features = [col for col in data.columns if col != target_variable]\n",
    "\n",
    "# Determine the proportions for train, test, and validation sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Split the data into train, test, and validation sets\n",
    "train_data, remaining_data = train_test_split(data, test_size=1 - train_ratio, random_state=42)\n",
    "test_data, validation_data = train_test_split(remaining_data, test_size=validation_ratio / (test_ratio + validation_ratio), random_state=42)\n",
    "\n",
    "# Set the y and X values for train, test, and validation sets\n",
    "y_train = train_data[target_variable]\n",
    "X_train = train_data[input_features]\n",
    "\n",
    "y_test = test_data[target_variable]\n",
    "X_test = test_data[input_features]\n",
    "\n",
    "y_validation = validation_data[target_variable]\n",
    "X_validation = validation_data[input_features]\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "# Verify the shapes of the resulting sets\n",
    "print(\"Train set shapes - X:\", X_train_scaled.shape, \"y:\", y_train.shape)\n",
    "print(\"Test set shapes - X:\", X_test_scaled.shape, \"y:\", y_test.shape)\n",
    "print(\"Validation set shapes - X:\", X_validation_scaled.shape, \"y:\", y_validation.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a DNN and return the metrics\n",
    "def train_dnn(X_train, y_train, X_val, y_val, X_test, y_test, epochs, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['Precision','Recall'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Make predictions on validation data\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.round(y_val_pred)\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred = np.round(y_test_pred)\n",
    "\n",
    "    # Calculate metrics for validation data\n",
    "    val_cm = confusion_matrix(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Calculate metrics for test data\n",
    "    test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    weight_matrix = model.get_weights()\n",
    "\n",
    "    return (val_cm, val_f1, val_precision, val_recall, val_accuracy,\n",
    "            test_cm, test_f1, test_precision, test_recall, test_accuracy,\n",
    "            loss, weight_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A. Testing to find the best number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159066/159066 [==============================] - 161s 1ms/step - loss: 0.0053 - precision: 0.8675 - recall: 0.4587 - val_loss: 0.0045 - val_precision: 0.9660 - val_recall: 0.5114\n",
      "19884/19884 [==============================] - 10s 524us/step\n",
      "19884/19884 [==============================] - 11s 573us/step\n",
      "Epoch 1/2\n",
      "159066/159066 [==============================] - 165s 1ms/step - loss: 0.0054 - precision: 0.8813 - recall: 0.4882 - val_loss: 0.0041 - val_precision: 0.9044 - val_recall: 0.5222\n",
      "Epoch 2/2\n",
      "159066/159066 [==============================] - 149s 939us/step - loss: 0.0049 - precision: 0.8829 - recall: 0.5675 - val_loss: 0.0106 - val_precision: 0.9789 - val_recall: 0.3890\n",
      "19884/19884 [==============================] - 12s 584us/step\n",
      "19884/19884 [==============================] - 11s 538us/step\n",
      "Epoch 1/5\n",
      "159066/159066 [==============================] - 156s 978us/step - loss: 0.0052 - precision: 0.8786 - recall: 0.4705 - val_loss: 0.0061 - val_precision: 0.9972 - val_recall: 0.4286\n",
      "Epoch 2/5\n",
      "159066/159066 [==============================] - 157s 986us/step - loss: 0.0059 - precision: 0.8847 - recall: 0.5421 - val_loss: 0.0058 - val_precision: 0.9968 - val_recall: 0.3697\n",
      "Epoch 3/5\n",
      "159066/159066 [==============================] - 145s 913us/step - loss: 0.0053 - precision: 0.8774 - recall: 0.5243 - val_loss: 0.0074 - val_precision: 0.9903 - val_recall: 0.2449\n",
      "Epoch 4/5\n",
      "159066/159066 [==============================] - 147s 926us/step - loss: 0.0047 - precision: 0.8670 - recall: 0.5286 - val_loss: 0.0065 - val_precision: 0.9967 - val_recall: 0.3589\n",
      "Epoch 5/5\n",
      "159066/159066 [==============================] - 146s 920us/step - loss: 0.0049 - precision: 0.8704 - recall: 0.5404 - val_loss: 0.0105 - val_precision: 0.9964 - val_recall: 0.3337\n",
      "19884/19884 [==============================] - 11s 553us/step\n",
      "19884/19884 [==============================] - 10s 494us/step\n",
      "Epoch 1/8\n",
      "159066/159066 [==============================] - 147s 920us/step - loss: 0.0050 - precision: 0.8437 - recall: 0.4417 - val_loss: 0.0053 - val_precision: 0.8516 - val_recall: 0.5582\n",
      "Epoch 2/8\n",
      "159066/159066 [==============================] - 150s 945us/step - loss: 0.0047 - precision: 0.8870 - recall: 0.5321 - val_loss: 0.0059 - val_precision: 0.9787 - val_recall: 0.3866\n",
      "Epoch 3/8\n",
      " 98405/159066 [=================>............] - ETA: 1:23 - loss: 0.0058 - precision: 0.8708 - recall: 0.5207"
     ]
    }
   ],
   "source": [
    "# Specify hyperparameters\n",
    "epochs = [1,2,5,8,10]  # Number of epochs to train\n",
    "\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Initialize variables to store results\n",
    "loss_per_epoch = []\n",
    "val_metrics = []\n",
    "test_metrics = []\n",
    "\n",
    "# Iterate over different numbers of hidden layers\n",
    "for epoch in epochs:\n",
    "    # Train the DNN and obtain metrics\n",
    "    val_cm, val_f1, val_precision, val_recall, val_accuracy, \\\n",
    "    test_cm, test_f1, test_precision, test_recall, test_accuracy, \\\n",
    "    loss, weight_matrix = train_dnn(X_train_scaled, y_train, X_validation_scaled, y_validation, X_test_scaled, y_test, epoch, batch_size)\n",
    "    # Store the loss per epoch\n",
    "    loss_per_epoch.append(loss)\n",
    "\n",
    "    # Store the metrics\n",
    "    val_metrics.append((val_cm, val_f1, val_precision, val_recall, val_accuracy))\n",
    "    test_metrics.append((test_cm, test_f1, test_precision, test_recall, test_accuracy))\n",
    "\n",
    "# Print and plot the metrics\n",
    "for i, epoch in enumerate(epochs):\n",
    "    print(f\"Metrics for {epochs} epochs:\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    val_cm, val_f1, val_precision, val_recall, val_accuracy = val_metrics[i]\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(val_cm)\n",
    "    print(\"F1-score:\", val_f1)\n",
    "    print(\"Precision:\", val_precision)\n",
    "    print(\"Recall:\", val_recall)\n",
    "    print(\"Accuracy:\", val_accuracy)\n",
    "    print()\n",
    "\n",
    "    print(\"Test Metrics:\")\n",
    "    test_cm, test_f1, test_precision, test_recall, test_accuracy = test_metrics[i]\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(\"F1-score:\", test_f1)\n",
    "    print(\"Precision:\", test_precision)\n",
    "    print(\"Recall:\", test_recall)\n",
    "    print(\"Accuracy:\", test_accuracy)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of the above data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Model using the best epoch with different sampling ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166805/166805 [==============================] - 169s 1ms/step - loss: 0.0347 - precision: 0.9259 - recall: 0.8169 - val_loss: 0.0064 - val_precision: 0.8579 - val_recall: 0.7395\n",
      "19884/19884 [==============================] - 10s 517us/step\n",
      "19884/19884 [==============================] - 12s 601us/step\n",
      "Metrics for sampling ratio 0.05:\n",
      "Validation Metrics:\n",
      "Confusion Matrix:\n",
      "[[635327    102]\n",
      " [   217    616]]\n",
      "F1-score: 0.7943262411347518\n",
      "Precision: 0.8579387186629527\n",
      "Recall: 0.7394957983193278\n",
      "Accuracy: 0.9994986342104353\n",
      "Test Metrics:\n",
      "Confusion Matrix:\n",
      "[[635334     86]\n",
      " [   192    650]]\n",
      "F1-score: 0.82382762991128\n",
      "Precision: 0.8831521739130435\n",
      "Recall: 0.7719714964370546\n",
      "Accuracy: 0.9995630730736709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify hyperparameters\n",
    "epochs = 1  # Number of epochs to train\n",
    "batch_size = 32  # Batch size for training\n",
    "sampling_ratios = [0.05,0.1, 0.2, 0.3, 0.4]  # Sampling ratios to test\n",
    "#,0.1, 0.2, 0.3, 0.4\n",
    "# Initialize variables to store results\n",
    "loss_per_epoch = []\n",
    "val_metrics = []\n",
    "test_metrics = []\n",
    "\n",
    "# Iterate over different sampling ratios\n",
    "for ratio in sampling_ratios:\n",
    "    # Apply SMOTE to augment the training data\n",
    "    smote = SMOTE(sampling_strategy=ratio, random_state=42)\n",
    "    X_train_aug, y_train_aug = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    # Train the DNN and obtain metrics\n",
    "    val_cm, val_f1, val_precision, val_recall, val_accuracy, \\\n",
    "    test_cm, test_f1, test_precision, test_recall, test_accuracy, \\\n",
    "    loss, weight_matrix = train_dnn(X_train_aug, y_train_aug, X_validation_scaled, y_validation, X_test_scaled, y_test, epochs, batch_size)\n",
    "\n",
    "    # Store the loss per epoch\n",
    "    loss_per_epoch.append(loss)\n",
    "\n",
    "    # Store the metrics\n",
    "    val_metrics.append((val_cm, val_f1, val_precision, val_recall, val_accuracy))\n",
    "    test_metrics.append((test_cm, test_f1, test_precision, test_recall, test_accuracy))\n",
    "\n",
    "# Print the results\n",
    "for i, ratio in enumerate(sampling_ratios):\n",
    "    print(f\"Metrics for sampling ratio {ratio}:\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(val_metrics[i][0])\n",
    "    print(\"F1-score:\", val_metrics[i][1])\n",
    "    print(\"Precision:\", val_metrics[i][2])\n",
    "    print(\"Recall:\", val_metrics[i][3])\n",
    "    print(\"Accuracy:\", val_metrics[i][4])\n",
    "\n",
    "    print(\"Test Metrics:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(test_metrics[i][0])\n",
    "    print(\"F1-score:\", test_metrics[i][1])\n",
    "    print(\"Precision:\", test_metrics[i][2])\n",
    "    print(\"Recall:\", test_metrics[i][3])\n",
    "    print(\"Accuracy:\", test_metrics[i][4])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of the above data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
